# Docker

**What is a Docker?**
- Docker is a containerization platform for developing, packaging, shipping, and running applications.
- It provides the ability to run an application in an isolated environment called a container.
- Makes deployment and development efficient.

**What is a Container?**
- A way to package an application with all the necessary dependencies and configuration.
- It can be easily shared
- Makes deployment and development efficient.

**Main Components of Docker**

- **Docker File:** It is a simple text file with instructions to build an image.
- **Docker Image:** Single File with all the dep and lib to run the
program.
- **Docker Containers:** Instance of an image.
- **Docker Registry:** A Docker registry is a central repository for storing and distributing Docker images.
    - **Registry**: A Docker Registry is a service that stores Docker images. It acts as a storage and distribution mechanism for Docker images. Developers can push (upload) and pull (download) Docker images to and from registries.
    - **Repository**: A Docker Repository is a collection of related Docker images, often with different versions of the same application or service. Within a Docker Registry, repositories organize images. Each repository can contain multiple image versions, identified by tags (e.g., `nginx:latest`, `nginx:1.19`, etc.).

The Docker Registry is where Docker images are physically stored. Docker Repositories organize related images within a registry, making it easier to manage and distribute different versions of applications or services.

### **Docker Engine Installation on Linux**

To install the Docker Engine on RedHat/CentOS, you can use the following commands:

Using CentOS repository, we can install Docker Engine on RedHat. First, we need to set up the repository.

**Set up the repository**

```
[root@redhat01 ~]# lscpu | more      # Check the architecture
[root@redhat01 ~]# sudo yum install -y yum-utils
[root@redhat01 ~]# sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
```

**Install Docker Engine**

To install the latest version, run:
```
[root@redhat01 ~]# sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
[root@redhat01 ~]# systemctl start docker.service
[root@redhat01 ~]# systemctl status docker.service
[root@redhat01 ~]# docker -v
[root@redhat01 ~]# docker ps
```

Install Docker Engine on other supported platforms: [Official Documentation](https://docs.docker.com/engine/install/)

### Creating DEMO Project - React Webpp

**Prerequisites:** <br>
Install Node.js and Visual Studio Code on your system. (Mac in this example)

Now open VS Code. Open the folder where you want to create this project and open the integrated terminal.
```
# node -v                           # Check Node.js version
v20.9.0
# npx create-react-app testapp      # Create a React-based application
Need to install the following packages:
create-react-app@5.0.1
Ok to proceed? (y)
```
This will create a new folder named testapp in your project directory, and all application files will be inside this testapp folder.

```
$ cd testapp        # Change directory to testapp
$ npm start         # Run the application
Compiled successfully!
You can now view testapp in the browser.

Local:              http://localhost:3000
On Your Network:    http://192.168.1.128:3000

Note that the development build is not optimized.
To create a production build, use npm run build.

webpack compiled successfully.
```
You can view the application in the browser using the link provided in the output. To **stop** the project, press `Ctrl+C`.

To run this application successfully, the app needs `node_modules` which will be saved in the **testapp** folder. If this folder is not available, you can run `npm install` to get these node modules and then run `npm start`.

Only main files are deployed while deploying the application; **node_modules** are not deployed. To obtain all the required node modules, use `npm install`, and then run it.

### Creating a Dockerfile

Create a new file named `Dockerfile` in testapp. You can also install the Docker extension in VS Code for easy Dockerfile creation.

```
FROM node:20

WORKDIR /myapp

COPY . .

RUN npm install

EXPOSE 3000

CMD ["npm", "start"]
```
**`FROM node:20`** : You need to specify a base for the image. Since this is a web app, you can use **node** as a base image. When you use `FROM node`, it retrieves the latest image from the Docker Registry. If you want to specify a particular version, you need to provide the tag name along with the image, such as `FROM node:20`. You can find available versions on the Docker repository.

**`WORKDIR /myapp`** : By default, when this Dockerfile creates a container from the image, it starts in an empty directory. You need to set a working directory where you want to run the application. This command creates a folder named **myapp** inside the container and sets it as the working directory.

**`COPY . .`** : Since this Dockerfile is in your local **testapp** folder, you need to copy all files from the current directory on your system to the container's working directory. Since you have set the working directory as **/myapp**, you can directly use `COPY . .`. Alternatively, you can specify the source directory explicitly: `COPY . /myapp`.

**`RUN npm install`** : To run the web app, the container needs the `node_modules` folder. After copying all main files to the working directory, you need to run `npm install` inside the container to install the required node modules.

**`EXPOSE 3000`** : Node.js applications typically listen on port 3000 by default. You can specify this port for the application to listen on by using the `EXPOSE` command in the Dockerfile.

**`CMD ["npm", "start"]`** : The Dockerfile is used to create an image, not to run the application directly. So you can't use `RUN npm start`. Therefore, you specify the command to start your application inside the container using `CMD`. In this example, it's `npm start`. Commands are passed in the form of arrays inside double quotes and separated by commas in the `CMD` instruction.

### Creating Docker Image

Go to the folder where your Dockerfile is present and run:
```
# docker build .
```
This will build an image using the instructions provided in the Dockerfile.

Once the image is created, you can check the image using:
```
# docker image ls
REPOSITORY TAG      IMAGE ID       CREATED          SIZE
<none>     <none>   2f02e42127df   36 seconds ago   1.39GB
```

### Run and Manage Docker containers

Using the image, you can create multiple containers.

To create containers, you need the IMAGE ID, which you will get from the previous command output.
```
# docker run 2f02e42127df
Compiled successfully!

You can now view testapp in the browser.

Local:              http://localhost:3000
On Your Network:    http://172.17.0.2:3000

Note that the development build is not optimized.
To create a production build, use npm run build.

webpack compiled successfully.
compiled successfully.
webpack compiled successfully.
```
The application is successfully running inside the container. You can see the output. However, this will continue running indefinitely because it's a web app and not a simple program that terminates after outputting.

But if you try to reach the website using `http://localhost:3000`, it will display **This site can't be reached**. This is because earlier, you could access this URL from your local system browser when the application was running on your local system. Now, it's running inside a container hosted on top of your local system. To make the URL accessible outside the container, you need to bind the port between the container and the local system.

In one terminal, the application is still running. First, stop this running application. You can open a different terminal window and run:
```
# docker ps           # process status
CONTAINER ID    IMAGE           COMMAND                     CREATED         PORTS   NAMES
b45c4f9544e8    2f02e42127df    "docker-entrypoint. s..."   2 minutes ago
0/tcp   dreamy_wiles
```

Whenever you run containers, by default, it assigns some random, unique names to them. You can stop the containers using this name. In our example, it's `dreamy_wiles`.

```
# docker stop dreamy_wiles
dreamy_wiles
# docker ps
CONTAINER   ID  IMAGE   COMMAND CREATED TS  NAMES
#
```
Now that the application is stopped, get the image ID and perform the port binding. The application listens on port 3000 inside the container, so bind it with port 3000 outside the container using the following command:
```
docker run -p <system_port>:<container_port> image_id
```
```
# docker image ls
REPOSITORY TAG      IMAGE ID       CREATED          SIZE
<none>     <none>   2f02e42127df   36 seconds ago   1.39GB
# docker run -p 3000:3000 2f02e42127df
```
Once you run the container using the above command, try accessing the URL from your local browser; you should now be able to access it.

### Running Container in Detached Mode

Now, whenever you run the container, you are running it in the foreground, which means you cannot easily perform other operations. We can run containers in the background and continue using our terminal for other tasks.

First, check the running containers and stop them.
```
# docker ps
CONTAINER ID    IMAGE           COMMAND                     CREATED         PORTS                    NAMES
b45c4f9544e8    2f02e42127df    "docker-entrypoint. s..."   15 minutes ago
0.0.0.0:3000->3000/tcp   nostalgic_dhawan
# docker stop nostalgic_dhawan
nostalgic_dhawan
```

To run containers in detached mode, use `-d` before `-p` for port binding in the `docker run` command.
```
# docker run -d -p 3000:3000 2f02e42127df
49acb5a75896e0d17e585d82f182f08c9df283e1b4304c837fa93a65e83d3d7b
#
```
This generates an ID, and the containers run in the background. The terminal is free for other commands. You can check whether your container is running using `docker ps`.

### Running Multiple Containers from a Single Image

Now, as the container is running in the background, if you try to use the same command again to create another container, it will give an error because port 3000 is already in use by another running application.
```
# docker run -d -p 3000:3000 2f0242127df
8388a5f0e0b7e858696a99cb0b5577230a12a3295ae7a546717755b4c8cd458
docker: Error response from daemon: driver failed programming external connectivity on endpoint awesome_pa
re (a31c43842eb68bbe069dd529d0554727fe9b2e2b6b1f0c3f86942d42b5d04): Bind for 0.0.0.0:3000 failed: port is already allocated.
```

You can use different ports for binding on your system. The container listens on port 3000 only, but you can bind it to different ports on your local system.
```
# docker run -d -p 3001:3000 2f02e42127df
bd2f1fe19f07efeef68164933436fc987280888f3bf78fe7a465f78d7366e680
# docker run -d -p 3002:3000 2f02e42127df
8efbb15678ea666fecf05645fa7ae13ac86924360aecf0fa8e3cab1f643d9732
# docker ps
CONTAINER ID    IMAGE           COMMAND                     CREATED         PORTS                    NAMES
8efbb15678ea    2f02e42127df    "docker-entrypoint. s..."   9 seconds ago
0.0.0.0:3002->3000/tcp   angry_ptolemy
bd2f1fe19f07    2f02e42127df    "docker-entrypoint. s..."   17 seconds ago
0.0.0.0:3001->3000/tcp   inspiring_wozniak
49acb5a75896    2f02e42127df    "docker-entrypoint. s..."   9 minutes ago
0.0.0.0:3000->3000/tcp   festive_colden
```
Since containers are isolated from each other, all three containers can listen on the same port simultaneously.

### Manage Containers

- `docker ps`: Get the running containers.
- `docker ps -a`: Get all the containers.
- `docker stop <container_name>`: To stop the container.
- `docker logs <container_name>` : To get logs of the container.
- `docker rm <container_name1> <container_name2>`: To remove the containers, you can provide multiple names separated by spaces.
- `docker run python:latest`: To run an image as a container.
- `docker run -it <image>`: To run a container with an interactive terminal, use `-it`.
- `docker run -p 8080:80 nginx:latest`: To run a container with port binding, use `-p`.
- `docker run -d -p 8080:80 nginx:latest`: To run a container in detached mode, use `-d`.
- `docker run -d --rm -p 3001:3000 2f02e42127df`: Using `--rm` when you run the container, it will delete the container once you stop it; you will not have to manually delete the containers.
- `docker run -d --rm --name "mywebapp" -p 3001:3000 2f02e42127df`: Provide a custom name to the container using `--name`.
- `docker tag mywebapp:02 newmywebapp:03`: To change the name of the image.

### Manage Images

- `docker image ls`: Lists the available images.
- `docker build -t mywebapp:01 .`: Provide custom tags to the images using `-t`. The name format should be **name:version**. Every time you tag the image with a new version, it creates a new version of the image and doesn't replace the previous version.
- `docker rmi mywebapp:01`: To delete the image, use `rmi`.

### Update Your Project

Whenever you make changes or updates to your project, there's no need to update the Dockerfile. Just build an image by providing a new tag with a new version number for understanding. In the following example, we are updating the `mywebapp` image to version 02.

```
$ docker image ls
REPOSITORY  TAG   IMAGE ID      CREATED         SIZE
mywebapp    01    2f0242127df   3 hours ago     1.39GB
$ docker build -t mywebapp:02 .
=> exporting to image
=> => exporting layers
=> => naming to docker.io/library/mywebapp:02
$ docker image ls
REPOSITORY  TAG   IMAGE ID      CREATED         SIZE
mywebapp    02    4a94f8428c0   12 seconds ago  1.39GB
mywebapp    01    2f0242127df   3 hours ago     1.39GB
$ docker run -d --rm --name "mywebapp" -p 3001:3000 mywebapp:02
ce028b14495821a781f18623fec260cb0f58dd14b0223139bd63b36066a945b4
```
You can also run another container using the previous image, but you need to provide a different container name and different local system port.

```
$ docker run -d --rm --name "mywebapp01" -p 3002:3000 mywebapp:01
55d47e1a1b9de6fd811c2b02c02f865a921d379aa4861f2bba5f1bf575a5e55e
```
Now both versions will be running, allowing you to compare both applications side by side.

### Pre-Defined Docker Images

There are pre-defined Docker images on Docker Hub. You can directly pull these images into your local system using the following command:

```
$ docker pull python
```
In this example, we are using the Python image. You need to go to Docker Hub, search for the image you want to use, and click on that image. You will see the command to pull the image to your local system.

By default, it pulls the latest version. If you need a specific version, you should provide the version number after the image using `:`. You can find these version details in the Docker repository.

### Docker Container With Interactive Mode

If you have a Python project that requires or expects input from the user to run, you can run a container in interactive terminal mode using `-it`.

**Example:**
myapp.py
```python
print('Program to sum two numbers')
num1 = int(input('Enter the first no '))
num2 = int(input('Enter the second no '))

result = num1 + num2

print(f'Sum of two numbers is {result}')
```
Create a Dockerfile to run the code:

```
FROM python

WORKDIR /myapp

COPY myapp.py .

CMD ["python", "myapp.py"]
```
Build and run the application:

```
$ docker build .                    # build image
$ docker image ls                   # list images    
$ docker run -it 5232aabae815       # run container with interactive mode
```

### Sharing Images to Docker Hub

You can store images in Docker Hub. First, create a repository in Docker Hub. Then, push the images into Docker Hub with tags.

On your local system, if you have a Dockerfile, first build an image with the same name as the Docker Hub repository name:

```
$ docker build -t philippaul/webapp-demo:latest .
$ docker images
REPOSITORY              TAG       IMAGE ID          CREATED         SIZE
philippaul/webapp-demo  latest    4a94f8428c0a      17 hours ago    1.39GB
```
If you already have an image created, you can rename it directly using `docker tag <old_image_name> <new_image_name>` and use that image to push.

First, login to Docker Hub using `docker login`, providing your username and password when prompted. Then, push the image using the image name:

```
$ docker login

Username: philippaul
Password:
Login Succeeded
$ docker push philippaul/webapp-demo:latest
The push refers to repository [docker.io/philippaul/webapp-demo]
ade1634289: Pushed
3983a69a132: Mounted from library/node
b136eed5154e: Mounted from library/python
latest: digest: sha256:bcod440045dc4dc96c521d2fddc145641417c0c2413f2504678acdbd560e8a71
```

### Pull Docker Images Remotely

If your image is public on Docker Hub, anyone can pull the image remotely and run it as a container on their system:

```
$ docker images                 # check images
$ docker pull philippaul/webapp-demo:latest     # pull images
$ docker run -p 3000:3000 philippaul/webapp-demo:latest # run container with port binding
```

### Docker Volumes

A Docker Volume is a mechanism used by Docker to persist data generated by and used by Docker containers. It provides a way to manage and store data separately from the container itself, ensuring data persistence even if the container is stopped or deleted. Docker Volumes provide a flexible and efficient way to manage data in Docker containers, ensuring data persistence, sharing, and ease of management across different environments and containers.

When you run a container, it will not store the data in separate volumes; the data is stored inside the container. Once the work is done and the container is deleted, the data is also deleted.
```
$ docker run -it --rm --name mypythonapp c193b38e7050
$ docker ps -a
CONTAINER ID IMAGE  COMMAND CREATED STATUS  PORTS   NAMES
$
```
You can use `-v` to add a volume to the container, specifying the **volume name** and **path** where you want to store the data. Now, when you run the container, even after the container is deleted, your data will remain in the volume you added. You can reuse this volume with another container.
```
$ docker run -it --rm -v myvolume:/myapp/ c193b38e7050
$ docker volume ls
DRIVER  VOLUME NAME
local   myvolume
$ docker volume inspect myvolume
[
    {
        "CreatedAt": "2023-11-06T17:51:012",
        "Driver": "local",
        "Labels": null,
        "Mountpoint": "/var/lib/docker/volumes/myvolume/_data",
        "Name": "myvolume",
        "Options": null,
        "Scope": "local"
    }
]
$ docker volume rm myvolume
$
```

### Mount Binds in Docker

In Docker, "mount binds" refer to a specific type of volume that allows you to mount a file or directory from the host machine into a Docker container. This means that the container can access and manipulate files or directories on the host system directly through this mount point.

Syntax: When using Docker CLI or Docker Compose, you specify a mount bind using the `-v` or `--volume` option, followed by the absolute path on the host and the path inside the container. For example:
```
$ docker run -v /host/path:/container/path ...
```
Here, `/host/path` is a directory or file on the host, and `/container/path` is the corresponding path inside the container.

**Example:** <br>
You have an application that depends on data from the `servers.txt` file for execution. You can keep updating this `servers.txt` file locally as needed. However, once you create a Docker image and run the container, the file is inside the container and cannot be easily updated. By using a mount bind, you can link the local `servers.txt` file with the `servers.txt` file inside the container. This way, any updates made to the local file will reflect inside the container as well.
```
$ docker run -v /Users/John/Project/servers.txt:/myapp/servers.txt --rm c193b38e7050
```
After `-v`, provide the **absolute path of the local file** you want to bind and after the `:`, provide the **file path inside the container**.

This method does not create physical volumes; it simply binds the local file with the file inside the container.

### .dockerignore

Similar to `.gitignore`, `.dockerignore` allows you to specify files that should not be pushed while building Docker images.

Usually, in a Dockerfile, we use `COPY . .`, which copies all files from the local directory to the image. This may inadvertently include files that should not be included in the image.

To address this, create a file named `.dockerignore` in your project folder where all your files are located. List the names of files you want Docker to ignore when building the image.

### Communication From/To Containers

Sometimes the program running inside the container may depend on other websites, databases, or applications running in different containers. To establish connections between these dependencies and the container, you can use Docker Networking.

#### 1. Working with APIs

We have a sample Python program that uses an API call to fetch random cat facts.

**api_demo.py**
```python
import requests

def fetch_random_cat_fact():
    url = "https://meowfacts.herokuapp.com/"

    try:
        response = requests.get(url)
        response.raise_for_status()  # Check for any HTTP errors

        fact = response.text
        return fact
    except requests.exceptions.RequestException as e:
        print(f"An error occurred: {e}")
        return None

def main():
    fact = fetch_random_cat_fact()
    if fact:
        print("Random Cat Fact:")
        print(fact)

if __name__ == "__main__":
    main()
```
Create a Dockerfile to build an image:

**Dockerfile**
```
FROM python

WORKDIR /myapp

COPY ./api_demo.py .

RUN pip install requests

CMD ["python", "api_demo.py"]
```
We have used the `requests` package in the Python file. Whenever we use an external package not included by default in Python, we need to install it before running the Python program. Use `RUN` to pass the installation command.

Now build the image and run the container:
```
$ docker build -t mypythonapp .
$ docker images
REPOSITORY    TAG        IMAGE ID        CREATED         SIZE
mypythonapp   latest     63d5dfdf76d1    8 seconds ago   1.03GB
$ docker run mypythonapp
```
The container will be able to connect to the API to fetch the random cat facts.

#### 2. Communication between Container & Local DB

We have a sample Python program that connects to MySQL, which is hosted on the local system.

**sample.py**
```python
import pymysql

def create_connection():
    return pymysql.connect(
        host="localhost",
        user="root",
        password="rootroot",
        database="userinfo"
    )

...
```
When this is run in a container, it will try to connect to `host="localhost"`, but it will result in a communication error between the MySQL server and localhost. Since the container is an isolated environment, it does not recognize `localhost` for connecting. You need to update the MySQL host details to `host.docker.internal` to establish a connection between the container and the localhost database.

**Updated sample.py**
```python
...

def create_connection():
    return pymysql.connect(
        host="host.docker.internal",
        user="root",
        password="rootroot",
        database="userinfo"
    )

...
```
#### 3. Communication between Containers

We have two containers: one running MySQL and another with code that will access the MySQL container.

**Pull and run the MySQL container**: This MySQL container requires you to pass a few environment variables while running it. These requirements will be mentioned in the Docker Hub documentation for the MySQL image.

If the required environment variables are not passed while running, the container will stop automatically. You can check the logs using `docker logs mysql`. The logs will indicate the variables that need to be specified.
```
$ docker pull mysql
$ docker images
$ docker run -d --env MYSQL_ROOT_PASSWORD="root" --env MYSQL_DATABASE="userinfo" --name mysqldb mysql
$ docker ps
$ docker inspect mysqldb            # Copy the IP address of the container
```
Now the MySQL container is up and running. For the other container, update the code with MySQL database details, build an image, and use the password and database name used while creating and running the MySQL container. For host details, use `docker inspect mysqldb` to get the container's details. In the **"Networks"** section, you will find the container's IP address, which will be the host.

**sql_demo.py**
```python
import pymysql

def create_connection():
    return pymysql.connect(
        host="172.17.8.2",
        user="root",
        password="root",
        database="userinfo"
    )

...
```
Create a Dockerfile and build the image.

**Dockerfile**
```
FROM python

WORKDIR /myapp

COPY ./sql_demo.py .

RUN pip install pymysql
RUN pip install cryptography

CMD ["python", "sql_demo.py"]
```
We are installing the **cryptography** package, as it is required for `SHA256_password` used by MySQL for authentication purposes.

```
$ docker build .
$ docker images
$ docker run -it --rm aaf8ed60a990
```
Now this container application will be able to take inputs from the user and save them in the MySQL container. Even if you stop the container and start it again, all the data will be in place.

### Creating Docker Network

Docker Network is a feature in Docker that enables containers to communicate with each other and with the external world. It provides a way to configure how containers interact within the same host or across different hosts, ensuring seamless communication and data transfer.

In the example above, we used the MySQL container's IP address in another container to establish connectivity between the containers. However, if we create a Docker Network, containers can interact with each other using just the container name. Docker will resolve the IP address automatically since both containers are on the same Docker Network.

First, you need to create a Docker Network.

```
$ docker network create my-net
987F783914538fbc7f4816adb5e4e86087d3309bc179342d96888e0d1cc03a74
$ docker network ls
NETWORK ID      NAME    DRIVER  SCOPE
987f78391453    my-net  bridge  local
$
```
Now, while running the MySQL container, you need to pass `--network <network_name>`.
```
$ docker run -d --env MYSQL_ROOT_PASSWORD="root" --env MYSQL_DATABASE="userinfo" --name mysqldb --network my-net mysql
```
In the Python code of another container, instead of using the MySQL container's IP address, you can directly use the container name `mysqldb`. Docker will resolve the hostname to its IP address within the Docker Network.

**sql_demo.py**
```python
import pymysql

def create_connection():
    return pymysql.connect(
        host="mysqldb",
        user="root",
        password="root",
        database="userinfo"
    )

...
```
Now you can build the image and run the container, specifying the network name.

```
$ docker build .
$ docker images
$ docker run -it --rm --network my-net b9b6a3faa994
```
### Docker Compose

Docker Compose is a configuration file based on YAML to manage single or multiple containers running on the same machine.

To run a container, we usually use `docker run`, often with multiple arguments like environment variables, remove, container name, interactive mode, etc. If we want to run multiple containers with different arguments, we have to use these long commands multiple times. This can be problematic when sharing the image with another teammate, who would need to know all the arguments to run the image correctly.

Example:
```
$ docker run -d --env MYSQL_ROOT_PASSWORD="root" --env MYSQL_DATABASE="userinfo" --name mysqldb --network my-net mysql
```

To simplify this, we can create a Docker Compose file and provide all the arguments there. You can then use a single command to run multiple containers at once.

You can create a Docker Compose file anywhere; just navigate to that folder before running `docker-compose`.

**docker-compose.yml**
```yml
services:
  mysqldb:                              # Provide service name
    image: 'mysql:latest'               # Provide image name and tag [optional]
    environment:                        # Provide --env variables
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_DATABASE=userinfo
    container_name: "mysqldb"           # Provide --name containername
```
All the arguments you pass in a command while running a container are now included in the YAML file. You can provide multiple service names to create and run multiple containers.

```
$ docker-compose up
```
This will pull the images and start the containers mentioned in the Docker Compose file. You need to open another terminal to check the running containers and start using them.

```
$ docker images
$ docker ps
$ docker-compose down               # To stop the containers
```
`docker-compose down` not only stops the containers but also removes them. There is no need to use `--rm` separately, but the images will remain.

```
$ docker-compose up -d
[+] Building 0.0s (0/0)                             docker: desktop-linux
[+] Running 2/2
Network pythondemoproject_default   Created                          0.0s
Container mysqldb                   Started                          0.0s
$ docker images
$ docker ps
```
When you run `docker-compose up` in detached mode using `-d`, it will start the containers and run them in the background. The terminal will be free to use, and there's no need to open another terminal.

> **Note:** **docker-compose** is not a replacement for **Dockerfile**. You stil require **Dockerfile** to build a custom image.

### docker-compose with Multiple Containers

Using `docker-compose`, we ran the **mysqldb** container. Now, we need to add the **mypythonapp** container to the Docker Compose file so that we can run both using a single Docker Compose file.

```yml
services:
  mysqldb:
    image: 'mysql:latest'
    environment:
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_DATABASE=userinfo
    container_name: "mysqldb"
    healthcheck:
      test: ['CMD', 'mysqladmin', 'ping','-h', 'localhost']
      timeout: 20s
      retries: 10

  mypythonapp:
    build: ./
    container_name: mypythonapp
    depends_on:
      mysqldb:
        condition: service_healthy
    stdin_open: true
    tty: true
```
- `healthcheck:`: Uses healthcheck to verify if the container is up and ready for connections. In this example, we use `CMD` to ping the localhost for 20 seconds to check the container's readiness. The **mypythonapp** container will depend on the **mysqldb** container being in a healthy `condition` before starting.
- `build: ./`: Specifies the path of the Dockerfile used to build the image. You can use a relative path; an absolute path is not required.
- `depends_on:`: Adds a dependency or condition. In this example, the **mypythonapp** container will only start if the **mysqldb** container is up and healthy.
- `stdin_open` and `tty`: Enable interactive terminals. In this example, **mypythonapp** needs user input to run, so these parameters are used.

Now, when you run `docker-compose up`, it will start both containers. It will wait for the **mysqldb** container to be up and healthy before running the **mypythonapp** container, which will then wait for user input.

You can also run specific containers from the Docker Compose file using the service names provided.

```
$ docker-compose run -d mysqldb
$ docker-compose run mypythonapp          # Not using "-d" as it requires an interactive terminal, and we have also enabled the same in the Docker Compose file.
```

You can refer to the Docker Compose [Official Documentation](https://docs.docker.com/compose/compose-file/) for syntax and parameters to be used in a Docker Compose file.

Once you run `docker-compose up`, you need to check the created images with `docker images` and the created containers with `docker ps -a`. If any container is in a stopped state, use `docker logs <container_name>` to check the logs of that container.